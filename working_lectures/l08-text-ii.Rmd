---
title: "BST430  Lecture 8"
subtitle: "Text Analysis"
author: "Tanzy Love, based on the course by Andrew McDavid"
institute: "U of Rochester"
date: "2021-09-26 (updated: `r Sys.Date()` by TL)"
output:
  xaringan::moon_reader:
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    lib_dir: libs
    seal: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ['ur-title', 'center', 'middle']
      ratio: "3:2"
---
  
```{r child = "setup.Rmd"}
```

```{r, include = FALSE}
library(tidyverse)
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1,
  pillar.print_max = 12,
  pillar.print_min = 8,
  pillar.max_footer_lines = 3,
  pillar.width = 65,
  width = 65)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE)
```

Here's the [R code in this lecture](l07/text-ii.R)

Here's the [books](l07/data/books.csv) 

*You have to have a good filepath to each dataset*


---

class: middle

![](https://raw.githubusercontent.com/juliasilge/tidytext/master/man/figures/tidytext.png)

Text mining using `tidytext`

---

## Text mining using `tidytext`

Text is inherently high-dimensional and noisy data.  We could spend weeks on this. Instead, we'll have to be content to know what we don't know:

*  Sampling text data and its potential ascertainment biases
*  Handling non-Roman (ASCII) characters
*  Parsing into tokens
*  Filtering low-content words
*  Dimension reduction, e.g., latent Dirichlet allocation or non-negative matrix factorization
*  Embeddings using pre-trained neural networks

Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/).

---

## Most important functionality

*  `unnest_tokens()` split a string into tokens (words, bi-grams, etc) as a data frame
*  `bind_tf_idf` calculate term and inverse-document frequencies.
*  `cast_sparse` convert to a (sparse) document-term matrix.

---

## Austen vs Kafka
```{r}
library(tidytext); library(stopwords); library(gutenbergr)
book_names = tibble(gutenberg_id = c(158, 105, 5200, 7849),
                    title = c('Emma', 'Persuasion',
                              'Metamorphosis', 'The Trial'))
books = gutenberg_download(book_names$gutenberg_id) %>% 
  left_join(book_names)
table(books$title)

```

---

## Austen vs Kafka - text

.scroll-box-15[
```{r, output.lines = 24}
books %>% group_by(title) %>% 
  slice_head(n=6)
```
]

---

## Get words

```{r}
book_words = unnest_tokens(books, text, output = 'word', drop = TRUE) #drops the original variable
book_words
```

---

## Count words by book

```{r}
word_counts = book_words %>%
  group_by(title) %>% count(title, word) %>%
  arrange(desc(n))
word_counts %>% slice_head(n = 4)
```

---

## Remove "stop" words

Stop words are common, low-semantic value words.  Sometimes useful to remove.

```{r}
word_counts %>% anti_join(get_stopwords()) %>% slice_head(n = 4)
```

---

## Term frequency in Kafka vs Austen

```{r}
total_words = word_counts %>%
  group_by(title) %>%
  summarize(total = sum(n))
word_counts = left_join(word_counts, total_words)
word_counts
```

---

## Term frequency in Kafka vs Austen


```{r plottf, warning = FALSE}

ggplot(word_counts, aes(x = n/total)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~title, scales = "free_y") + 
  theme_minimal()
```

---

## Zipf's law
Distributions like those on the previous slide are typical in language.  A classic version of this relationship is called Zipf's law.

> Zipf's law states that the frequency that a word appears is inversely proportional to its rank.

---

## Zipf's law

.panelset[

.panel[.panel-name[Code]
```{r freq_by_rank, fig.show='hide'}
freq_by_rank = word_counts %>%
  group_by(title) %>%
  mutate(rank = row_number(),
         `term frequency` = n/total) %>%
  ungroup()

freq_by_rank %>%
  ggplot(aes(x = rank, y = `term frequency`, color = title)) +
  geom_abline(intercept = -0.62, slope = -1,
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10() +
  theme_minimal()
```
]
.panel[.panel-name[Plot]
```{r ref.label = 'freq_by_rank', echo = FALSE}
```
]
]
---

## Sentiment analysis

```{r}
sentiments
```

---

## Add sentiments to words

```{r}
word_sentiments = word_counts %>%
  mutate(counts = n) %>%
  left_join(sentiments, relationship = "many-to-many") %>% 
                                    #multiple matches here
  filter(!is.na(sentiment)) %>%     #already grouped!
  mutate(word_collapse = fct_lump_n(word, n = 10, w = counts),) %>%
    #it needs to be told that n are the weights of these words
    select(title, word_collapse, sentiment, counts)
word_sentiments
```

---

##  Which is more happy?

```{r sentiment_comparison}
ggplot(word_sentiments, aes(y = fct_reorder(word_collapse,  counts, .fun = sum), 
                            x = counts, fill = sentiment)) + 
  geom_col() + facet_wrap(~title, scales = 'free_x') + 
  ylab("Word") + xlab("Occurrence") + theme_minimal()
```

---



## Term frequency and inverse document frequency

The inverse document frequency is

$$\text{idf}(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$

The IDF thus ranges from 0 for words that appear in every document up to $log(n)$ for a word unique across documents.

The term frequency is just the word counts, normalized to the number of words per text, so the popular TF-IDF<sup>1</sup> metric is just

$$\text{tf-idf}(\text{term}) = \text{idf}(\text{term}) \times \text{tf}(\text{term})$$
.footnote[[1] Popular, and curiously devoid of an obvious statistical model.  [Some attempts to link to information theory](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Justification_of_idf) have been made.]
---

## Calculate TF-IDF


```{r calc-tf, output.lines = 12}
word_counts = word_counts %>% bind_tf_idf(word, title, n)
word_counts
```

---

## TF-IDF of Kafka and Austen

This works relatively well to identify signature words -- some represent content,  some represent author style (e.g. contractions used by Kafka)

```{r signature_words, echo = FALSE}
word_counts %>% group_by(title) %>% slice_max(tf_idf, n = 15) %>% ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~title, scales = "free") +
  theme_minimal()
```

---

## Occurrence matrix

Lastly, we might want to convert our counts to an occurrence matrix $\mathbf X = [x_{ij}]$ where $x_{ij}$ is the number of times document $i$ contains term $j$.

Most $x_{ij}$ will be zero, reflecting Zipf's law.  We will almost always want to store it in a special format called a .alert[sparse matrix], that only stores the non-zero entries and their index in the matrix.

---

## `cast_sparse()`

```{r}
X = cast_sparse(word_counts, title, word, n)
class(X)
dim(X)
sum(X>0)
```

This is useful for downstream modeling, such as latent Dirichlet allocation.

---

## what is `cast_sparse()`

```{r}
X[,10001:10004]
```

```{r}
rowSums(as.matrix(X))
```

<!-- --- -->

<!-- # Resources -->

<!-- Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/). -->

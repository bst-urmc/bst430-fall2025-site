---
title: "BST430  Lecture 7"
subtitle: "Text Processing"
author: "Tanzy Love, based on the course by Andrew McDavid"
institute: "U of Rochester"
date: "2021-09-26 (updated: `r Sys.Date()` by TL)"
output:
  xaringan::moon_reader:
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    lib_dir: libs
    seal: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ['ur-title', 'center', 'middle']
      ratio: "3:2"
---
  
```{r child = "setup.Rmd"}
```

```{r, include = FALSE}
library(tidyverse)
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1,
  pillar.print_max = 12,
  pillar.print_min = 8,
  pillar.max_footer_lines = 3,
  pillar.width = 65,
  width = 65)
knitr::opts_chunk$set(cache = TRUE)
```

Here's the [R code in this lecture](l07/text.R)

Here's the [cardiac-dx](l07/data/cardiac-dx.csv) and 
[sample_sheet2](l07/ae/data/sample_sheet2.csv)


*You have to have a good filepath to each dataset*

---

## Cardiac diagnoses

```{r, output.lines = 13}
diagnoses = read_csv('l07/data/cardiac-dx.csv')
diagnoses
```

.question[How to get all the diagnoses of hypoplastic left heart?]

---

## Attempt 1

```{r}
diagnoses %>% count(diagnoses) %>% 
  arrange(desc(n))
```

--

Too many unique diagnoses to make headway with this.

---

## Using string matching

```{r}
filter(diagnoses, str_detect(diagnoses, 'hypoplastic'))
```

.question[What different sort of hypoplasties are there?]

---

## Split into pieces

```{r,  output.lines = 10}
diagnoses_row = diagnoses %>% 
  tidyr::separate_rows(diagnoses, 
                       sep = " \\| ") #WTH? #<< 
diagnoses_row
```

---

## Split into pieces

```{r}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  count(diagnoses) %>% arrange(desc(n))
```

---

## Plot co-occurrence

.panelset[

.panel[.panel-name[Code]
```{r dx0, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  ggplot(aes(y = diagnoses, x = as.factor(id))) + 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx0',  echo = FALSE, out.width="75%"}
```
]
]

---

### Sensible factor orders

.panelset[

.panel[.panel-name[Code]
```{r dx-tile, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + #<< 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile',  echo = FALSE, out.width="75%"}
```
]
]

---

### Wrap text width

.panelset[

.panel[.panel-name[Code]
```{r dx-tile-wrap, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  mutate(diagnoses = str_wrap(diagnoses, width = 40)) %>% #<<
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile-wrap',  echo = FALSE, out.width="75%"}
```
]
]

---

### Adjust text size and justification

.panelset[

.panel[.panel-name[Code]
```{r dx-tile-wrap-just, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  mutate(diagnoses = str_wrap(diagnoses, width = 40)) %>%
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + 
  geom_tile() + 
  theme(axis.text.y = element_text(hjust = 0, vjust = 0, size = 8)) + #<<
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile-wrap-just',  echo = FALSE, out.width="75%"}
```
]
]

---

# Topics

*  Why text processing
*  Low level processing:
  *  concatenate, count characters, substring, split strings.
*  Regular expressions (aka regex):
  *  detect, extract, replace
*  Text mining
  *  Tokenizing, filtering, analysis

---

## Low level text processing

*  concatenate with `stringr::str_c()` and `glue::glue()`
*  count characters with `base::nchar()`
*  extract and replace substrings with `stringr::str_sub()`
*  split with `str_split_fixed()` (generally) or `str_split()` (less often)

---

## Packages
```{r, include = FALSE}
library(stringr)
library(glue)
```

.pull-left[
`stringr` and `glue` rationalize much of text processing, which is otherwise a bit of a thicket in R.
]
.pull-right[
![](https://raw.githubusercontent.com/tidyverse/stringr/master/man/figures/logo.png)
![](https://raw.githubusercontent.com/tidyverse/glue/master/man/figures/logo.png)


]

---

## Concatenate strings

.pull-left[
```{r}
names = c("Jeff B.", "Larry E.", "Warren B.")
favorite_food = c("caviar", "cake", "Pappy Van Winkle")
str_c(names, 
      " likes ", #note additional spaces
      favorite_food, ".")
```
]
.pull-right[
```{r}
dinner = glue::glue("{names} likes {favorite_food}.")
dinner
```
]


---

## Some special characters

*  \n newline
*  \r carriage return
*  \t tab
*  \f form feed
*  \Unnnnnnnn  Unicode character with given code
*  \\\ literal backslash (.alert[this one will prove to be especially annoying...])
*  \" literal quote

Others are listed in `?'"'` (the help on the quote function).

---

## Glue with newlines and unicode

```{r}
glue::glue("{names} \n {favorite_food} \U1F643.")
```


---

## Count characters

```{r}
names
nchar(names)
```

---

## Extract substrings

.pull-left[
Extract
```{r}
str_sub(dinner, 1, 11)
```
]
.pull-right[
Replace
```{r}
str_sub(dinner, 
        #space + l-
        nchar(names) + 2, 
        #space + l-i-k-e
        nchar(names) + 6 
) = "demands"
dinner
```
]

---

## split strings

Get a character matrix, padding / collapsing excess fields.
```{r}
str_split_fixed(dinner, " ",  4)
```

```{r}
str_split_fixed(dinner, " ", 6)
```

---

## split strings

Get exactly what you ask for.
```{r}
str_split(dinner, " ")
```

Also recall `tidyr::separate` and `tidyr::separate_rows`.

---

## Other handy low-level string manipulations

*  Change case `str_to_lower()`, `str_to_upper()`, `str_to_title()`
*  Remove trailing/leading `str_trim()` or repeated `str_squish()` whitespace
*  Wrap long lines `stringr::str_wrap()`
*  Truncate `str_trunc()` or abbreviate `base::abbreviate()` long strings.

---

## Application Exercise

.hand[Let's try.] 

Read some data
```{r read-samplesheet}
data = read_csv("l07/ae/data/sample_sheet2.csv")
data
```
---
Split the `folder_name` field by the '_' character.  This will give you a character `matrix`.

--

```{r split-folder}
char_matrix = str_split_fixed(data$folder_name, pattern = "_", n = 6)
char_matrix
```

---
Using this and `str_c`, recreate the fields `treatment`, `sample_ID`, `tissue_source`.
You can extract various columns using `char_matrix[,integer_of_the_column_you_want]`.  

--

```{r}
data %>% select(folder_name) %>% 
  mutate(treatment = char_matrix[,2],
         sample_ID = str_c(char_matrix[,2], "_", char_matrix[,3]),
         tissue_source = char_matrix[,4])
```

---
Convert the `dataset` field to be entirely lowercase with `str_to_lower`.

--

```{r}
data %>% select(dataset) %>% 
  mutate(new_data = str_to_lower(dataset))
```

---
Collapse sample_ID field into a single character vector (length 1) separated by semicolons ";" using `str_c(..., collapse = ...)`.

--

```{r}
str_c(data$sample_ID, collapse=";")
```

---

class: middle

.hand[Regular Expressions]: 

---

## Regular expressions

> Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems.
-- Jamie Zawinski (creator of Mozilla)

*  Are like *find-replace*, *wildcards* \* 
*  Are found in nearly every computer language
*  Can be just the ticket to solving *some* problems

---
class: bg-green

## Syntax

Write what you want to match (if it's alpha-numeric).

```{r}
lunch = c("one app", "two appetizers", "three apples")
str_view_all(lunch, 'apple')
```

---
class: bg-green

## Match multiple things: wildcard

`.` is a generic wildcard, matches any character.

```{r}
str_view_all(lunch, 'app.')
```

---
class: bg-green

## Match multiple things: character class

`[<set>]` is a character class, matches all characters in `<set>`.  Specify a range of characters with `[<char>-<char>]`.  Invert a class with `[^<set>]`.

```{r}
str_view_all(lunch, 'app[le]')
```

---
class: bg-green

## Match multiple things: disjunction

`(<x>|<y>)` is a disjunction, matches `<x>` or `<y>`.

```{r}
str_view_all(lunch, 'app(le|etizer)s')
```

---
class: bg-green

## Qualifiers modify matches

1. `*` zero or more matches
2. `?` zero or one matches
3. `+` one or more matches
4. `{min,max}` to match between min-max times.

Compare back to `"app."`, which didn't match the first string.
```{r}
str_view_all(lunch, 'app.*')
```

---

## Match without consuming with zero-width identifiers

*  `^` matches a zero-width "character" present at the start of all lines.
*  `$` is the analogous character at the end
*  `\b` is between "words".  

For example, the string:

`red tired`

can be thought as

.darkgreen[^\b]red.darkgreen[\b] .darkgreen[\b]tired.darkgreen[\b$]

---
class: bg-green

## Require word boundary

We must double the `\` to keep R from interpreting it as an escape character.

```{r}
str_view_all("red tired", "\\bred\\b")
```


---
class: bg-green


## Match unconditionally

```{r}
str_view_all("red tired", "red")
```

---


## Using regular expressions

*  Test for an expression `str_detect()`.
*  Return first `str_extract()` or all `str_extract_all()` matching portions of string.
*  Return first `str_match()` or all `str_match_all()` matching portions of string **and capture groups**.
*  Replace first `str_replace()`  or all `str_replace_all()` matching portions of string and capture groups.

---

## `str_detect()`

```{r}
str_detect(string = c("A", "AA", "AB", "B"), 
           pattern = "A")
str_detect(string = lunch, pattern = 'app.')
```

---

## `str_extract()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_extract(string = feline, pattern = "cat")
str_extract_all(string = feline, pattern = "cat")
```

---


## `str_match()`

For simple queries, behaves like `str_extract()`
```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_match(feline, "cat")
```

---

## `str_match()`

But returns **capture groups** `(<expression>)` separately.

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_match(feline, "(\\w*) cat.? (\\w*)")
```

`\w = [A-Za-z0-9_]`, we must double the `\` to keep R from interpreting it as an escape character.

---

## `str_match_all()`
```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_match_all(feline, "(\\w*) cat.? (\\w*)")

```

---

## `str_replace()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_replace(feline, "cat", "murder machine")
```

---

## `str_replace_all()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_replace_all(feline, "cat", "murder machine")
```

---

## `str_replace()` also can use capture groups

Use `\1` to refer to the first capture group, `\2` for the second, etc.  Note the `\\` because `\` must be escaped in R.
```{r}
str_replace_all(feline, "(\\w*)", "\\1\\1")
```

Actually, any regular expression can use a capture group, both for matching and replacing!
---

## Application exercise

.hand[Let's try]. 

If devtools works for you, you might also experiment with installing `devtools::install_github("gadenbuie/regexplain")`.  See https://www.garrickadenbuie.com/project/regexplain/ for example usage.

We'll use the dataset `words`, which is loaded automatically when you load the `stringr` package.  It contains `r length(words)` common English words.


```{r}
word_df = tibble(word = words)
```

---

How many words contain an "x" anywhere in them? List them.

--

```{r}
filter(word_df, str_detect(word, "x"))
```

---

How many words end in "x"?  List them.  Use `$` to match the end of the string.

--

```{r}
filter(word_df, str_detect(word, "x$"))
```

---

Do any words start with "x"?  Use `^` to match the start of the string.

--

```{r}
filter(word_df, str_detect(word, "^x"))
```

---

Using wildcards `.` and quantifiers `+` (rather than the results of the previous exercises), find all the words that contain "x" in the interior (but not at the start or end).  Check that the number of results from 1-4 are coherent.

--

```{r}
filter(word_df, str_detect(word, '.+x.+'))
```

---

On average, how many vowels are there per word?  (Hint: use `str_count` and `[]` to define a character class).  What is the average vowel-per-letter (# of vowels normalized per length)

--

```{r}
word_df %>% 
  mutate(vowels = str_count(word, "[aeiouAEIOU]"),
         vowels_per_letter = vowels/nchar(word)) %>% 
  summarise(mean(vowels), mean(vowels_per_letter))
```

---
List all the words with three or more vowels in a row.  Use `{min_matches,max_matches}` as a quantifier.

--

```{r}
filter(word_df, str_detect(word, '[aeiou]{3,10}'))
```

---
## Sentences

Now, consider the `r nrow(sentences)` in the `sentences` data set:

```{r}
sentence_df = tibble(sentence  = sentences)
sentence_df
```

---
Extract the first word from each sentence.  Hint: negate the space character class "[ ]" to match everything except a space.

--

```{r}
sentence_df %>% mutate(first_word = str_extract(sentence, '\\w*')) %>% head(3)# doesn't include apostrophes '''
sentence_df %>% mutate(first_word = str_extract(sentence, '([^ ])*'))
```

---
Return all the sentences that contain the colors "red", "blue" or "green".  Use the `|` disjunction.

--

```{r}
filter(sentence_df, str_detect(sentence, ("red|blue|green")))
```

---
Extract the first word ending in "s".  Use a capture group `()`, `str_match()` and the everything-but-space `[^ ]` character class.

--

```{r}
sentence_df = sentence_df %>% 
  mutate(ends_in_s = str_match(sentence, "([^ ]*s)\\b")[,2])
sentence_df
```


---
(Stretch goal) Notice that two questions ago, we also matched the sentence

> The colt reared and threw the tall rider.

because "reared" contains "red". Fix the regular expression so it only matches the complete words, not just a fragment using the "\b" word start marker.  Hint:  use "\\\\b" to keep the backslash from being used as an escape character.
--

```{r}
filter(sentence_df, str_detect(sentence, ("\\bred\\b|\\bblue\\b|\\bgreen\\b")))
```


<!-- --- -->

<!-- class: middle -->

<!-- ![](https://raw.githubusercontent.com/juliasilge/tidytext/master/man/figures/tidytext.png) -->

<!-- Text mining using `tidytext` -->

<!-- --- -->

<!-- ## Text mining using `tidytext` -->

<!-- Text is inherently high-dimensional and noisy data.  We could spent weeks on this. Instead, we'll have to be content to know what we don't know: -->

<!-- *  Sampling text data and its potential ascertainment biases -->
<!-- *  Handling non-Roman (ASCII) characters -->
<!-- *  Parsing into tokens -->
<!-- *  Filtering low-content words -->
<!-- *  Dimension reduction, e.g., latent Dirichlet allocation or non-negative matrix factorization -->
<!-- *  Embeddings using pre-trained neural networks -->

<!-- Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/). -->

<!-- --- -->

<!-- ## Most important functionality -->

<!-- *  `unnest_tokens()` split a string into tokens (words, bi-grams, etc) as a data frame -->
<!-- *  `bind_tf_idf` calculate term and inverse-document frequencies. -->
<!-- *  `cast_sparse` convert to a (sparse) document-term matrix. -->

<!-- --- -->

<!-- ## Austin vs Kafka -->
<!-- ```{r} -->
<!-- library(tidytext) -->
<!-- book_names = tibble(gutenberg_id = c(158, 1342, 5200, 7849), -->
<!--                     title = c('Emma', 'Pride and prejudice', -->
<!--                               'Metamorphosis', 'The Trial')) -->
<!-- books = gutenbergr::gutenberg_download(book_names$gutenberg_id) %>% left_join(book_names) -->
<!-- ``` -->

<!-- .scroll-box-10[ -->
<!-- ```{r, output.lines = 24} -->
<!-- books %>% group_by(title) %>% slice_head(n=6) -->
<!-- ``` -->
<!-- ] -->

<!-- --- -->

<!-- ## Get words -->

<!-- ```{r} -->
<!-- book_words = unnest_tokens(books, text, output = 'word', drop = TRUE) -->
<!-- book_words -->
<!-- ``` -->

<!-- --- -->

<!-- ## Count words by book -->

<!-- ```{r} -->
<!-- word_counts = book_words %>% -->
<!--   group_by(title) %>% count(title, word) %>%  -->
<!--   arrange(desc(n)) -->
<!-- word_counts %>% slice_head(n = 3) -->
<!-- ``` -->

<!-- --- -->

<!-- ## Remove "stop" words -->

<!-- Stop words are common, low-semantic value words.  Sometimes useful to remove. -->

<!-- ```{r} -->
<!-- word_counts %>% anti_join(get_stopwords()) %>% slice_head(n = 3) -->
<!-- ``` -->

<!-- --- -->

<!-- ## Term frequency in Kafka vs Austin -->

<!-- ```{r} -->
<!-- total_words = word_counts %>%  -->
<!--   group_by(title) %>%  -->
<!--   summarize(total = sum(n)) -->
<!-- word_counts = left_join(word_counts, total_words) -->
<!-- word_counts  -->
<!-- ``` -->

<!-- --- -->

<!-- ## Term frequency in Kafka vs Austin -->


<!-- ```{r plottf, warning = FALSE} -->
<!-- ggplot(word_counts, aes(n/total)) + -->
<!--   geom_histogram(show.legend = FALSE) + -->
<!--   xlim(NA, 0.0009) +  -->
<!--   facet_wrap(~title, ncol = 2, scales = "free_y") + theme_minimal() -->
<!-- ``` -->
<!-- --- -->

<!-- ## Zipf's law -->
<!-- Distributions like those on the previous slide are typical in language.  A classic version of this relationship is called Zipf's law.  -->

<!-- > Zipf's law states that the frequency that a word appears is inversely proportional to its rank.  -->

<!-- --- -->

<!-- ## Zipf's law -->

<!-- .panelset[ -->

<!-- .panel[.panel-name[Code] -->
<!-- ```{r freq_by_rank, fig.show='hide'} -->
<!-- freq_by_rank = word_counts %>%  -->
<!--   group_by(title) %>%  -->
<!--   mutate(rank = row_number(),  -->
<!--          `term frequency` = n/total) %>% -->
<!--   ungroup()  -->

<!-- freq_by_rank %>%  -->
<!--   ggplot(aes(x = rank, y = `term frequency`, color = title)) +  -->
<!--   geom_abline(intercept = -0.62, slope = -1,  -->
<!--               color = "gray50", linetype = 2) + -->
<!--   geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +  -->
<!--   scale_x_log10() + -->
<!--   scale_y_log10() +  -->
<!--   theme_minimal() -->
<!-- ``` -->
<!-- ] -->
<!-- .panel[.panel-name[Plot] -->
<!-- ```{r ref.label = 'freq_by_rank', echo = FALSE} -->
<!-- ``` -->
<!-- ] -->
<!-- ] -->
<!-- --- -->

<!-- ## Sentiment analysis -->

<!-- ```{r} -->
<!-- word_sentiments = word_counts %>%  -->
<!--   left_join(sentiments) %>% #<< -->
<!--   filter(!is.na(sentiment)) %>%  -->
<!--   group_by(title) %>%  -->
<!--   mutate(word_collapse = fct_lump_n(word, n = 10, w = n), -->
<!--     word_collapse = fct_reorder(word_collapse, n, sum)) %>%  -->
<!--     select(title, word_collapse, sentiment, n) -->
<!-- word_sentiments -->
<!-- ``` -->

<!-- --- -->

<!-- ##  Which is more happy? -->

<!-- ```{r} -->
<!-- ggplot(word_sentiments, aes(y = fct_reorder(word_collapse,  n, .fun = sum), x = n, fill = sentiment)) + geom_col() + facet_wrap(~title, scales = 'free_x') + ylab("Word") + xlab("Occurrence") + theme_minimal() -->
<!-- ``` -->

<!-- --- -->



<!-- ## Term frequency and inverse document frequency -->

<!-- The inverse document frequency is -->

<!-- $$\text{idf}(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$ -->

<!-- The IDF thus ranges from 0 for words that appear in every document up to $log(n)$ for a word unique across documents. -->

<!-- The term frequency is just the word counts, normalized to the number of words per text, so the popular TF-IDF<sup>1</sup> metric is just -->

<!-- $$\text{tf-idf}(\text{term}) = \text{idf}(\text{term}) \times \text{tf}(\text{term})$$ -->
<!-- .footnote[[1] Popular, and curiously devoid of an obvious statistical model.  [Some attempts to link to information theory](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Justification_of_idf) have been made.] -->
<!-- --- -->

<!-- ## Calculate TF-IDF -->


<!-- ```{r calc-tf, output.lines = 12} -->
<!-- word_counts = word_counts %>% bind_tf_idf(word, title, n) -->
<!-- word_counts -->
<!-- ``` -->

<!-- --- -->

<!-- ## TF-IDF of Kafka and Austen -->

<!-- This words relatively well to identify signature words -- some represent content,  some represent author style (e.g. contractions used by Kafka) -->

<!-- ```{r, echo = FALSE} -->
<!-- word_counts %>% group_by(title) %>% slice_max(tf_idf, n = 15) %>% ungroup() %>% -->
<!--   mutate(word = reorder(word, tf_idf)) %>% -->
<!--   ggplot(aes(tf_idf, word)) + -->
<!--   geom_col(show.legend = FALSE) + -->
<!--   labs(x = "tf-idf", y = NULL) + -->
<!--   facet_wrap(~title, ncol = 2, scales = "free") +  -->
<!--   theme_minimal() -->
<!-- ``` -->

<!-- --- -->

<!-- ## Occurrence matrix -->

<!-- Lastly, we might want to convert our counts to an occurrence matrix $\mathbf X = [x_{ij}]$ where $x_{ij}$ is the number of times document $i$ contains term $j$. -->

<!-- Most $x_{ij}$ will be zero, reflecting Zipf's law.  We will almost always want to store it in a special format called a .alert[sparse matrix], that only stores the non-zero entries and their index in the matrix. -->

<!-- --- -->

<!-- ## `cast_sparse()` -->

<!-- ```{r} -->
<!-- X = cast_sparse(word_counts, title, word, n) -->
<!-- class(X) -->
<!-- dim(X) -->
<!-- sum(X>0) -->
<!-- ``` -->

<!-- This is useful for downstream modeling, such as latent Dirichlet allocation. -->

<!-- --- -->

<!-- # Resources -->

<!-- Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/). -->

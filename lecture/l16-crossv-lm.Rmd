---
title: "BST430  Lecture 16"
subtitle: "Multivariate linear models"
author: "Tanzy Love, based on the course by Andrew McDavid"
institute: "U of Rochester"
date: "2021-11-07 (updated: `r Sys.Date()` by TL)"
output:
  xaringan::moon_reader:
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    lib_dir: libs
    seal: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ['ur-title', 'center', 'middle']
      ratio: "3:2"
---
  
```{r child = "setup.Rmd"}
```

```{r, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, warnings=FALSE)
library(tidyverse)
library(tidymodels)
library(openintro)
library(patchwork)
library(skimr)
library(conflicted)
set.seed(1234)
options(
  warnPartialMatchArgs = FALSE,
  warnPartialMatchAttr = FALSE, 
  warnPartialMatchDollar = FALSE,
  width = 100
)
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)
conflict_prefer("step", "recipes")
```

  
# Agenda

0.  Multivariate plots
1.  More modeling syntax
3.  Diagnostics
4. Prediction

---

class: code70

### `mtcars`

- For this lesson, we will use the (infamous) `mtcars` dataset that comes
  with R by default.
```{r, message=FALSE}
library(tidyverse)
library(broom)
data("mtcars")
glimpse(mtcars)
```

???

1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles.

[, 1]	mpg	Miles/(US) gallon
[, 2]	cyl	Number of cylinders
[, 3]	disp	Displacement (cu.in.)
[, 4]	hp	Gross horsepower
[, 5]	drat	Rear axle ratio
[, 6]	wt	Weight (1000 lbs)
[, 7]	qsec	1/4 mile time
[, 8]	vs	Engine (0 = V-shaped, 1 = straight)
[, 9]	am	Transmission (0 = automatic, 1 = manual)
[,10]	gear	Number of forward gears
[,11]	carb	Number of carburetors

---

class: middle

.hand[Visualizing multivariate relationships]

---

### Exploratory data analyses of multivariate data

-  Hard
-  Necessary
-  Often generates quite a few plots before you identify one that you can keep

---

### One simple trick

Pairs plots!

.panelset[
.panel[.panel-name[Code]
```{r pairs1, fig.show="hide"}
library(GGally)
GGally::ggpairs(mtcars[c(1,2,4,6,8,9,10)])
```

]
.panel[.panel-name[Plot]
```{r ref.label = "pairs1", echo = FALSE, warning = FALSE, out.width = "70%"}
```

]
]

---

* Evidently `vs`, `am`, `gear` and maybe `cyl` should be cast to factors

.panelset[
.panel[.panel-name[Code]
```{r pairs2, fig.show="hide"}
mtcars = mtcars %>% mutate(across(c(vs, am, gear, cyl), factor))
GGally::ggpairs(mtcars[c(1,2,4,6,8,9,10)])
```

]
.panel[.panel-name[Plot]
```{r ref.label = "pairs2", echo = FALSE, warning = FALSE, out.width = "70%"}
```

]
]

---

- Let's suppose we wanted to determine which variables affect fuel 
  consumption (the `mpg` variable in the dataset).
- `cyl`, `disp`, `hp`, `drat`, `wt` all appear to be correlated with `mpg`
- `vs`, `am` and possibly `gear` could have distributional differences, as well

---
class: code70

- To begin, we'll look at the association between the variables log-weight 
  and mpg

```{r}
ggplot(mtcars, aes(x = wt, mpg)) +
  geom_point() +
  scale_x_continuous(transform = "log") + xlab("Weight") + ylab("Miles Per Gallon")
```
    
- It seems that log-weight is negatively associated with mpg.
- It seems that the data approximately fall on a line.

---

### OLS fits in R

1. Make sure you have the explanatory variables in the format you want:
```{r}
submt = mtcars %>% mutate(logwt = log(wt))
```
2. Use `lm()`
```{r}
lmout = lm(mpg ~ logwt, data = submt)
lmtide = tidy(lmout)
select(lmtide, term, estimate)
```

---

### `lm` syntax

```{r, eval=FALSE}
lm(response ~ pred1 + pred2*pred3, data = data) 
```
Finds the OLS estimates of the following model: 

> response = $\beta_0+ \beta_1\text{pred1} + \beta_2\text{pred2} + \beta_{3}\text{pred3}+ \beta_{23}\text{pred2} * \text{pred3}$ + error
  
The `data` argument tells `lm()` where to find the response and 
  explanatory variables.

---

###  Formula syntax

*  `x:z` form the interaction between `x` and `z` -- this is element-by-element multiplication if at least `x` or `z` is continuous, otherwise it is an outer-product.
*  `x*z` form interactions and include main effects. Equivalent to `x + z + x:z`
*  `+ 0` or `- 1` exclude an intercept term, or `- x` exclude `x` if included otherwise.
*  `(x + z + u)^2` form all two-way interactions and main effects with `x`, `z`, `u`.
*  `I(x - 10)` or `I(x^2)` -- perform arithmetic operations that use `+`, `-`, `*`, `^`, etc, on a variable "on-the-fly" in the formula.   
  * better than just transform before you model
*  `y ~ .` every variable in the data, except the response `y`.

---
class: code50

### Example

.alert[But don't do this without thinking!]

```{r}
tidy(lm(mpg ~ ., data = mtcars))
```


For a predictive goal, probably should use some method that does shrinkage and basis expansion.  For inference, need to consider putative casual models.


---

# Prediction (Interpolation)

- **Interpolation**: Making estimates/predictions within the range of the data.
- **Extrapolation**: Making estimates/predictions outside the range of the data.
- Interpolation is fine. Extrapolation is dangerous.
.question[why?]

---

- Interpolation
```{r, echo = FALSE}
ggplot(data = submt, mapping = aes(x = logwt, y = mpg)) +
    geom_point() +
    geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
    geom_segment(data = data.frame(x    = 1, 
                                   xend = 1,
                                   y    = 0,
                                   yend = coef(lmout)[1] + 1 * coef(lmout)[2]),
                 mapping = aes(x = x, xend = xend, y = y, yend = yend), 
                 lty = 2, color = "red", lwd = 1) +
    geom_hline(yintercept = 0, lty = 2, alpha = 1/2) +
    ylab("MPG") +
    xlab("Log-weight")
```

---

- Extrapolation
```{r, echo = FALSE}
ggplot(data = submt, mapping = aes(x = logwt, y = mpg)) +
    geom_point() +
    geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
    geom_segment(data = data.frame(x    = 3, 
                                   xend = 3,
                                   y    = 0,
                                   yend = coef(lmout)[1] + 3 * coef(lmout)[2]),
                 mapping = aes(x = x, xend = xend, y = y, yend = yend), 
                 lty = 2, color = "red", lwd = 1) +
    geom_hline(yintercept = 0, lty = 2, alpha = 1/2) +
    ylab("MPG") +
    xlab("Log-weight")
```

---

## Why is extrapolation dangerous?

1. Not sure if the linear relationship is the same outside the range of
   the data (because we don't have data there to see the relationship).
2. Not sure if the variability is the same outside the range of the 
   data (because we don't have data there to see the variability).

---

### Make a prediction:

1. You need a data frame with the exact same
   variable name as the explanatory variable. 
```{r}
newdf = tribble(~logwt,
                 1, 
                 1.5)
```
2. Then you use the `predict()` function to obtain predictions.
```{r}
newdf = newdf %>%
mutate(predictions = predict(object = lmout, newdata = newdf))
```

---

## Assumptions and Violations

- In the linear model, you can trade assumptions for inference:

- Assumptions in *decreasing* order of importance
  1. **Independence** - The knowledge of the value of one observation does not 
     give you any information on the value of another.
  2. **Linearity** - The relationship looks like a straight line.
  3. **Equal Variance** - The spread is the same for every value of $x$
  4. **Normality** - The distribution of the errors isn't too skewed and there aren't 
     any *too* extreme points. (Only an issue if you have outliers and a 
     small number of observations because thanks be to the 
     [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).
  5. **Fixed and Known** predictors

---

### What do we lose when violated?

  1. **Linearity** violated - Linear regression line does not pick up actual 
     conditional expectation.  As a linear approximation, the results will be sensitive to the particular $x$ sampled.
  2. **Independence** violated - Linear regression line is unbiased, but standard 
     errors can be badly off.
  3. **Equal Variance** violated - Linear regression line is unbiased, but standard 
     errors are off. Your $p$-values may be too small, or too large.
  4. **Normality** violated - Only an issue if your sample size is "small".  Unstable results if outliers are present.  Your $p$-values may be too small, or too large.
  5. **Fixed and Known** violated if there is measurement error in the predictors 
     
.question[Why are no assumptions made about the distribution of the 
  explanatory variable (the $x_i$'s)?]

---
  
## Evaluating Independence

- Think about the problem.
  - Were different responses measured on the same observational/experimental unit?
  - Were data collected in groups?

- Non-independence: The temperature today and the temperature 
  tomorrow. If it is warm today, it is probably warm tomorrow.

-  Non-independence: You are measuring properties of 500 single cells isolated from 3 mice.  Because the cells within a given mouse are probably similar, each cell is not independent.  

---

### xkcd 2533

```{r, echo = FALSE}
knitr::include_graphics("l16/img/slope_hypothesis_testing_2x.png")

```

Via [xkcd](https://xkcd.com/2533/)



---
class: middle

# Interpreting coefficients when you log

---

### Log `x` 

Generally, when you use logs, you interpret associations on a 
  *multiplicative* scale instead of an *additive* scale.

No log:
- Model: $E[y_i] = \beta_0 + \beta_1 x_i$
- Observations that differ by 1 unit in $x$ tend to differ by $\beta_1$ units in $y$.

Log $x$:
- Model: $E[y_i] = \beta_0 + \beta_1 \log_2(x_i)$
- Observations that are twice as large in $x$ tend to differ by $\beta_1$ units in $y$.

---

### log `y` 

Log $y$:
- Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 x_i$
- Observations that differ by 1 unit in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 

Log both:
- Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 \log_2(x_i)$
- Observations that are twice as large in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 

.footnote[Note: we commit statistical abuse here, since $\exp \left[ \text{E}(\log(Y) | X) \right] \neq \text{E}(Y | X)$, ie, `exp` doesn't commute through the expectation. Though the delta method says this is the 1st order approximation.
]


---

# Summary of R commands

- `augment()`:
  - Residuals $r_i = y_i - \hat{y}_i$: `$.resid`
  - Fitted Values $\hat{y}_i$: `$.fitted`
- `tidy()`:
  - Name of variables: `$term`
  - Coefficient Estimates: `$estimate`
  - Standard Error (standard deviation of sampling distribution of coefficient estimates): `$std.error`
  - t-statistic: `$statistic`
  - p-value: `$p.value`
- `glance()`: 
  - R-squared value (proportion of variance explained by regression line, higher is better): `$r.squared`
  - AIC (lower is better): `$AIC`
  - BIC (lower is better): `$BIC`

---

# Prediction

---

## Goal: Building a spam filter

- Data: Set of emails and we know if each email is spam/not and other features 
- Use logistic regression to predict the probability that an incoming email is spam
- Optimize our model by picking the model with the best predictive performance

--
- Building a model to predict the probability that an email is spam is only half of the battle! We also need a decision rule about which emails get flagged as spam (e.g. what probability should we use as out cutoff?)

--
- A simple approach: choose a single threshold probability and any email that exceeds that probability is flagged as spam

---
class: middle

# Logistic regression

<!-- --- -->

<!-- ## Generalized Linear Models (GLMs) -->

<!-- - Logistic regression is a *generalized linear model(GLM)* used to model a binary categorical outcome using numerical and categorical predictors -->

<!-- - A GLM is an extension of the least squares linear model $E(Y_i|\mathbf{x}_i) = \mathbf{x}_i \beta$, $\text{Var}(Y|\mathbf{x}_i) = \sigma^2$ to -->
<!-- $$g(E(Y_i|x_i)) = \eta_i = \mathbf{x}_i \beta$$ -->
<!-- , $Y|X \sim$ Exponential-Family, and $g$ is known as a **link** function. -->

<!-- - Whereby the glm allows non-linear effects in $\beta$, and since the conditional distribution of $Y|X$ can include all sorts of interesting families, it also permits non-constant residual variance. -->

<!-- --- -->

<!-- ### The Bernoulli GLM with logit link -->

<!-- - To finish specifying the Logistic model we just need to define a reasonable link function that connects $\eta_i$ to $p_i$: logit function -->

<!-- -- -->
<!-- - **Logit function:** For $0\le p \le 1$ -->

<!-- $$logit(p) = \log\left(\frac{p}{1-p}\right)$$ -->


---

## Logit function, visualised

```{r echo=FALSE}
d = tibble(p = seq(0.001, 0.999, length.out = 1000)) %>%
  mutate(logit_p = log(p/(1-p)))

ggplot(d, aes(x = p, y = logit_p)) + 
  geom_line() + 
  ylab("logit(p)") +
  labs(title = "logit(p) vs. p") + 
  scale_x_continuous(labels = c(min(d$p), .25, .5, .75, max(d$p)))
```

---

## Properties of the logit

- The logit function takes a value between 0 and 1 and maps it to a value between $-\infty$ and $\infty$

--
- Inverse logit (aka expit) function takes real values back to the probability space
<!-- $$g^{-1}(x) = \frac{\exp(x)}{1+\exp(x)} = \frac{1}{1+\exp(-x)}$$ -->

--
- The expit function takes a value between $-\infty$ and $\infty$ and maps it to a value between 0 and 1

--
- This formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success -- more on this later

<!-- --- -->

<!-- ## The logistic regression model -->

<!-- - Based on the three GLM criteria we have -->
<!--   - $y_i|x_i \sim \text{Bern}(p_i)$ -->
<!--   - $\text{logit}(p_i) = \eta_i$ -->
<!--   - $\eta_i = \beta_0+\beta_1 x_{1,i} + \cdots + \beta_n x_{n,i}$ -->

<!-- -- -->
<!-- - From which we get -->

<!-- $$p_i = \frac{\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}{1+\exp(\beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}$$ -->

---

## Modeling spam

In R we fit a GLM in the same way as a linear model except we

- use `"glm"` instead of `"lm"` as the engine 

- define `family = "binomial"` for the link function to be used in the model

--

- When using `tidymodels`, specify the model with `logistic_reg()`

---

## Prediction

- The mechanics of prediction is **easy**:
  - Plug in values of predictors to the model equation
  - Calculate the predicted value of the response variable, $\hat{y}$

--
- Getting it right is **hard**!
  - There is no guarantee the model estimates you have are correct
  - Or that your model will perform as well with new data as it did with your sample data

---

## Underfitting and overfitting

```{r echo=FALSE, out.width="70%", warning = FALSE}
lm_fit = linear_reg() %>%
  set_engine("lm") %>%
  fit(y4 ~ x2, data = association)

loess_fit = loess(y4 ~ x2, data = association)

loess_overfit = loess(y4 ~ x2, span = 0.05, data = association)

association %>%
  select(x2, y4) %>%
  mutate(
    Underfit = augment(lm_fit$fit) %>% select(.fitted) %>% pull(),
    OK       = augment(loess_fit) %>% select(.fitted) %>% pull(),
    Overfit  = augment(loess_overfit) %>% select(.fitted) %>% pull(),
  ) %>%
  pivot_longer(
    cols      = Underfit:Overfit,
    names_to  = "fit",
    values_to = "y_hat"
  ) %>%
  mutate(fit = fct_relevel(fit, "Underfit", "OK", "Overfit")) %>%
  ggplot(aes(x = x2)) +
  geom_point(aes(y = y4), color = "darkgray") +
  geom_line(aes(y = y_hat, group = fit, color = fit), size = 1) +
  labs(x = NULL, y = NULL, color = NULL) +
  scale_color_viridis_d(option = "plasma", end = 0.7)
```

---

## Spending our data

- Several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.

- Doing all of this on the entire data we have available can lead to **overfitting**

- Allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we've done so far)

---

class: middle

# Splitting data

---

## Splitting data

- **Training set:**
  - Sandbox for model building 
  - Spend most of your time using the training set to develop the model
  - Majority of the data (usually 80%)
  
- **Testing set:**
  - Held in reserve to determine efficacy of one or two chosen models
  - Critical to look at it once, otherwise it becomes part of the modeling process
  - Remainder of the data (usually 20%)
  
---

## Performing the split

```{r}
# Fix random numbers by setting the seed 
# Enables analysis to be reproducible when random numbers are used 
set.seed(20211115)

# Put 80% of the data into the training set 
email_split = initial_split(email, prop = 0.80)

# Create data frames for the two sets:
train_data = training(email_split)
test_data  = testing(email_split)
```

.font70[`training` / `testing` aren't doing anything very fancy -- just using `sample_n` and then taking its complement, but this approach generalizes to more complicated ways to split our data.]

---
class: code70

## Peek at the split

.small[
.pull-left[
```{r}
glimpse(train_data)
```
]
.pull-right[
```{r}
glimpse(test_data)
```
]
]

---

class: middle

# Modeling workflow

---

## Fit a model to the training dataset

```{r}
email_fit = logistic_reg() %>%
  set_engine("glm") %>%
  fit(spam ~ ., data = train_data, family = "binomial")
```

---

## Categorical predictors

```{r echo=FALSE, out.width="75%", fig.width=10}
factor_predictors = train_data %>%
  select(where(is.factor), -spam) %>%
  names()

p_to_multiple = ggplot(train_data, aes(x = to_multiple, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_from = ggplot(train_data, aes(x = from, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_sent_email = ggplot(train_data, aes(x = sent_email, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_winner = ggplot(train_data, aes(x = winner, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_format = ggplot(train_data, aes(x = format, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_re_subj = ggplot(train_data, aes(x = re_subj, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_urgent_subj = ggplot(train_data, aes(x = urgent_subj, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_number = ggplot(train_data, aes(x = number, fill = spam)) +
  geom_bar() +
  scale_fill_manual(values = c("#E48957", "#CA235F"))

p_to_multiple + p_from + p_sent_email + p_winner + p_format + p_re_subj + p_urgent_subj + p_number +
  plot_layout(ncol = 4, guides = "collect") & 
  theme(axis.title.y = element_blank())
```

---

## Fit a model to the training dataset

```{r}
email_fit = logistic_reg() %>%
  set_engine("glm") %>%
  fit(spam ~ . - from - sent_email - viagra, data = train_data, family = "binomial") #<<
```

.code50[
```{r}
email_fit
```
]

???

Dropping `from`, `sent_email` quells the warnings about predicted probabilities being 0/1. Actually some of these variables might have predictive power, but we would need a more sophisticated approach than logistic regression to use them

---

## Predict outcome on the testing dataset

```{r}
predict(email_fit, test_data)
```


---

## Predict probabilities on the testing dataset

```{r}
email_pred = predict(email_fit, test_data, type = "prob") %>%
  bind_cols(test_data %>% select(spam, time))

email_pred
```

---

## A closer look at predictions


```{r highlight.output=c(6, 10)}
email_pred %>%
  arrange(desc(.pred_1)) %>%
  print(n = 10)
```

---

## Evaluate the performance

**Receiver operating characteristic (ROC) curve**<sup>+</sup> which plot true positive rate vs. false positive rate (1 - specificity)

.pull-left[
```{r roc, fig.show="hide"}
email_pred %>%
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()
```
]
.pull-right[
```{r ref.label="roc", echo=FALSE, out.width="100%"}
```
]

.footnote[
.small[
<sup>+</sup>Originally developed for operators of military radar receivers, hence the name.
]
]

---

## Evaluate the performance

Find the area under the curve:

.pull-left[
```{r}
email_pred %>%
  roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```
]
.pull-right[
```{r ref.label="roc", echo=FALSE, out.width="100%"}
```
]


---

class: middle

# Feature engineering

---

## Feature engineering

- There are all sorts of ways to build predictive models of binary variables: random forests, support vector machines, neural networks, $k$ nearest neighbors, gradient boosting, ...

- In their own way, each learns the mapping $\hat f: \mathbf{x} \mapsto Y$

--
- But the variables $\mathbf{x}$ that go into the model and how they are represented are just as critical to success of the model

--
- **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance) 

-  How this engineering is done is part of the learned function $\hat f$, and needs to be accounted for when we evaluate our models.

---

## A simple approach: `mutate()`

```{r}
library(lubridate)
train_data %>%
  mutate(
    date = date(time),
    dow  = wday(time),
    month = month(time)
    ) %>%
  select(time, date, dow, month) %>%
  sample_n(size = 5) # shuffle to show a variety
```

???

time would be used a "seconds" since the epoch as a continuous variable, however this would not allow any flexibility about weekdays vs weekends, time of year, etc.

however, we would need to repeat this mutate on train / test independently, at a minimum this would not keep things D.R.Y.

It's actually even worse than that, in that some mutates (collapsing factors, scaling variables) have implicitly learned parameters -- if we don't account for this we can end up with data leaking between training and test or with garbage predictions in test


---

## Modeling workflow, revisited

- Create a **recipe** for feature engineering steps to be applied to the training data

--
- Fit the model to the training data after these steps have been applied

--
- Using the model estimates from the training data, predict outcomes for the test data

--
- Evaluate the performance of the model on the test data

---

class: middle

# Building recipes


???

Fancy mutate statements that can be applied easily to both training / test
Actually, is building up and saving a set of functions (without evaluating them)

---

## Initiate a recipe

```{r initiate-recipe, results="hide"}
email_rec = recipe(
  spam ~ .,          # formula
  data = train_data  # data to use for cataloguing names and types of variables
  )

summary(email_rec)
```

.code50[
```{r echo=FALSE}
summary(email_rec) %>% print(n = 21)
```
]

---

## Remove certain variables

```{r}
email_rec = email_rec %>%
  step_rm(from, sent_email)
```

.code70[
```{r echo=FALSE}
email_rec
```
]

---

## Feature engineer date

```{r}
email_rec = email_rec %>%
  step_date(time, features = c("dow", "month")) %>%
  step_rm(time)
```

.code70[
```{r echo=FALSE}
email_rec
```
]

---

## Discretize numeric variables

```{r}
email_rec = email_rec %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20))
```

.code70[
```{r echo=FALSE}
email_rec
```
]

---

## Create dummy variables

```{r}
email_rec = email_rec %>%
  step_dummy(all_nominal(), -all_outcomes())
```

.code70[
```{r echo=FALSE}
email_rec
```
]

---

## Remove zero variance variables

Variables that contain only a single value

```{r}
email_rec = email_rec %>%
  step_zv(all_predictors())
```

.code70[
```{r echo=FALSE}
email_rec
```
]

---

## All in one place

```{r}
email_rec = recipe(spam ~ ., data = email) %>%
  step_rm(from, sent_email) %>%
  step_date(time, features = c("dow", "month")) %>%               
  step_rm(time) %>%
  step_cut(cc, attach, dollar, breaks = c(0, 1)) %>%
  step_cut(inherit, password, breaks = c(0, 1, 5, 10, 20)) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors())
```

---

class: middle

# Building workflows

---

## Define model

```{r}
email_mod = logistic_reg() %>% 
  set_engine("glm")

email_mod
```

---

## Define workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.

```{r}
email_wflow = workflow() %>% 
  add_model(email_mod) %>% 
  add_recipe(email_rec)
```

.code60[
```{r echo=FALSE}
email_wflow
```
]

---

## Fit model to training data

```{r}
email_fit = email_wflow %>% 
  fit(data = train_data)
```

---

.code50[
```{r, output.lines=30}
tidy(email_fit)
```
]

---

## Make predictions for test data

```{r, R.options=list(width = 60), output.lines = 10}
email_pred = predict(email_fit, test_data, type = "prob") %>% 
  bind_cols(test_data) 

email_pred
```

---

## Evaluate the performance

.pull-left[
```{r roc-2, fig.show="hide"}
email_pred %>%
  roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()
```
]
.pull-right[
```{r ref.label="roc-2", echo=FALSE, out.width="100%"}
```
]

---

## Evaluate the performance

.pull-left[
```{r}
email_pred %>%
  roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```
]
.pull-right[
```{r ref.label="roc-2", echo=FALSE, out.width="100%"}
```
]

---

class: middle

# Making decisions

---

## Cutoff probability: 0.5

.panelset[
.panel[.panel-name[Output]

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.5**.

```{r ref.label = "confusion-50,", echo = FALSE}
```
]
.panel[.panel-name[Code]
```{r confusion-50, results = "hide"}
cutoff_prob = 0.5
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
    ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  knitr::kable(col.names = c("", "Email is not spam", "Email is spam"))
```
]
]

---

## Cutoff probability: 0.25

.panelset[
.panel[.panel-name[Output]

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.25**.

```{r ref.label = "confusion-25,", echo = FALSE}
```
]
.panel[.panel-name[Code]
```{r confusion-25, results = "hide"}
cutoff_prob = 0.25
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
    ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  knitr::kable(col.names = c("", "Email is not spam", "Email is spam"))
```
]
]

---

## Cutoff probability: 0.75

.panelset[
.panel[.panel-name[Output]

Suppose we decide to label an email as spam if the model predicts the probability of spam to be **more than 0.75**.

```{r ref.label = "confusion-75,", echo = FALSE}
```
]
.panel[.panel-name[Code]
```{r confusion-75, results = "hide"}
cutoff_prob = 0.75
email_pred %>%
  mutate(
    spam      = if_else(spam == 1, "Email is spam", "Email is not spam"),
    spam_pred = if_else(.pred_1 > cutoff_prob, "Email labelled spam", "Email labelled not spam")
    ) %>%
  count(spam_pred, spam) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  knitr::kable(col.names = c("", "Email is not spam", "Email is spam"))
```
]
]

---

## Evaluating performance on training data

-  The training set does not have the capacity to be a good arbiter of performance.

--
- It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

--
- Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.

.footnote[
.small[
Source: [tidymodels.org](https://www.tidymodels.org/start/resampling/)
]
]

---

class: middle

# Cross validation

---

## Cross validation

More specifically, **v-fold cross validation**:

- Shuffle your data v partitions
- Use 1 partition for validation, and the remaining v-1 partitions for training
- Repeat v times

.footnote[
.small[
You might also heard of this referred to as k-fold cross validation.
]
]

---

## Cross validation

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("l17/img/cross-validation.png")
```

---

## Split data into folds

.pull-left[
```{r}
set.seed(345)

folds = vfold_cv(train_data, v = 5)
folds
```
]
.pull-right[
```{r echo=FALSE, out.width="100%", fig.align="right"}
knitr::include_graphics("l17/img/cross-validation.png")
```
]

---
class: code60

## Fit resamples

.pull-left[
```{r, warning = FALSE}
set.seed(456)

email_fit_rs = email_wflow %>%
  fit_resamples(folds)

email_fit_rs
```
]
.pull-right[
```{r echo=FALSE, out.width="100%", fig.align="right"}
knitr::include_graphics("l17/img/cross-validation-animated.gif")
```
]

---

## Collect CV metrics

```{r}
collect_metrics(email_fit_rs)
```

---

## Deeper look into CV metrics

.panelset[
.panel[.panel-name[Raw]
```{r}
collect_metrics(email_fit_rs, summarize = FALSE) %>%
  print(n = 10)
```
]
.panel[.panel-name[Pretty]
```{r echo=FALSE}
collect_metrics(email_fit_rs, summarize = FALSE) %>%
  select(id, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  knitr::kable(col.names = c("Fold", "accuracy", "roc_auc", "brier_class"), digits = 3)
```
]
]


---

### Training data optimism

In the typical case, we use cross-validation to tune and select between models, then use the train/test split to evaluate the final chosen model

-  In this case, the training data does not severely overestimate the performance in testing or cross-validation.
-  In general, the gap between training and test is a function of the training *optimism*.
-  $\nearrow$ model flexibility, $\nearrow$ optimism
-  $\searrow$ training set size, $\nearrow$ optimism.

---

### Example of training optimism

.pull-left[
```{r, warning=FALSE}
set.seed(123)
train_data_small = train_data %>% sample_n(size = 100)
overfit = fit(email_wflow, data = train_data_small)
overpredict = predict(overfit, train_data_small, type = "prob") %>% 
  bind_cols(train_data_small)
```
]

.pull-right[
```{r, echo =FALSE, out.width="90%"}
 overpredict %>% roc_curve(
    truth = spam,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()

overpredict %>% roc_auc(
    truth = spam,
    .pred_1,
    event_level = "second"
  )
```
]

---

# Final announcements

*  You will have access to the lectures as long as github exists and provides hosting for websites.  
*  You will have access to your completed homework assignments on github as long as you have access to your github account.
*  I would rather that you not electronically redistribute the entirety of a graded assignment with another student<sup>1</sup> 

.footnote[[1] Reminder: sharing graded assignments within a class year isn't technically prohibited, but sharing across class years is prohibited and is a violation of the academic honesty policy per department policy.]






---

# Acknowledgments

Adapted from David Gerard's [Stat 512](https://data-science-master.github.io/lectures/05_linear_regression/05_simple_linear_regression.html)

Data science in a box [U4D7](https://rstudio-education.github.io/datascience-box/course-materials/slides/u4-d07-prediction-overfitting/u4-d07-prediction-overfitting.html#1) [U4D8](https://rstudio-education.github.io/datascience-box/course-materials/slides/u4-d08-feature-engineering/u4-d08-feature-engineering.html#1) [U4D9](https://rstudio-education.github.io/datascience-box/course-materials/slides/u4-d09-cross-validation/u4-d09-cross-validation.html#1)

## Resources

- Chapter 25 from [RDS](https://r4ds.had.co.nz/).


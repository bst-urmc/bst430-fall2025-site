---
title: "BST430  Lecture 10"
subtitle: "Text Processing"
author: "Andrew McDavid"
institute: "U of Rochester"
date: "2021-09-26 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    lib_dir: libs
    seal: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ['ur-title', 'center', 'middle']
      ratio: "3:2"
---
  
```{r child = "setup.Rmd"}
```

```{r, include = FALSE}
library(tidyverse)
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1,
  pillar.print_max = 12,
  pillar.print_min = 8,
  pillar.max_footer_lines = 3,
  pillar.width = 65,
  width = 65)
knitr::opts_chunk$set(cache = TRUE)
```


## Cardiac diagnoses

```{r, output.lines = 13}
diagnoses = read_csv('l10/data/cardiac-dx.csv')
diagnoses
```

.question[How to get all the diagnoses of hypoplastic left heart?]

---

## Attempt 1

```{r}
diagnoses %>% count(diagnoses) %>% 
  arrange(desc(n))
```

--

Too many unique diagnoses to make headway with this.

---

## Using string matching

```{r}
filter(diagnoses, str_detect(diagnoses, 'hypoplastic'))
```

.question[What different sort of hypoplasties are there?]

---

## Split into pieces

```{r,  output.lines = 10}
diagnoses_row = diagnoses %>% 
  tidyr::separate_rows(diagnoses, 
                       sep = " \\| ") #WTH? #<< 
diagnoses_row
```

---

## Split into pieces

```{r}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  count(diagnoses) %>% arrange(desc(n))
```

---

## Plot co-occurrence

.panelset[

.panel[.panel-name[Code]
```{r dx0, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  ggplot(aes(y = diagnoses, x = as.factor(id))) + 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx0',  echo = FALSE, out.width="75%"}
```
]
]

---

### Sensible factor orders

.panelset[

.panel[.panel-name[Code]
```{r dx-tile, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + #<< 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile',  echo = FALSE, out.width="75%"}
```
]
]

---

### Wrap text width

.panelset[

.panel[.panel-name[Code]
```{r dx-tile-wrap, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  mutate(diagnoses = str_wrap(diagnoses, width = 40)) %>% #<<
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + 
  geom_tile() + 
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile-wrap',  echo = FALSE, out.width="75%"}
```
]
]

---

### Adjust text size and justification

.panelset[

.panel[.panel-name[Code]
```{r dx-tile-wrap-just, fig.show = 'hide'}
diagnoses_row %>% filter(str_detect(diagnoses, 'hypoplastic')) %>%
  mutate(diagnoses = str_wrap(diagnoses, width = 40)) %>%
  ggplot(aes(y = fct_infreq(diagnoses), x = fct_infreq(as.factor(id)))) + 
  geom_tile() + 
  theme(axis.text.y = element_text(hjust = 0, vjust = 0, size = 8)) + #<<
  scale_x_discrete(breaks = NULL) +
  labs(y = "Diagnosis", x = 'Patient', 
       main = 'Co-occurrence of hypoplastic heart disorders')
```
].panel[.panel-name[Plot]
```{r, ref.label='dx-tile-wrap-just',  echo = FALSE, out.width="75%"}
```
]
]

---

# Topics

*  Why text processing
*  Low level processing:
  *  concatenate, count characters, substring, split strings.
*  Regular expressions (aka regex):
  *  detect, extract, replace
*  Text mining
  *  Tokenizing, filtering, analysis

---

## Low level text processing

*  concatenate with `stringr::str_c()` and `glue::glue()`
*  count characters with `base::nchar()`
*  extract and replace substrings with `stringr::str_sub()`
*  split with `str_split_fixed()` (generally) or `str_split()` (less often)

---

## Packages
```{r, include = FALSE}
library(stringr)
library(glue)
```

.pull-left[
`stringr` and `glue` rationalize much of text processing, which is otherwise a bit of a thicket in R.
]
.pull-right[
![](https://raw.githubusercontent.com/tidyverse/stringr/master/man/figures/logo.png)
![](https://raw.githubusercontent.com/tidyverse/glue/master/man/figures/logo.png)


]

---

## Concatenate strings

.pull-left[
```{r}
names = c("Jeff B.", "Larry E.", "Warren B.")
favorite_food = c("caviar", "cake", "Pappy Van Winkle")
str_c(names, 
      " likes ", #note additional spaces
      favorite_food, ".")
```
]
.pull-right[
```{r}
dinner = glue::glue("{names} likes {favorite_food}.")
dinner
```
]


---

## Some special characters

*  \n newline
*  \r carriage return
*  \t tab
*  \f form feed
*  \Unnnnnnnn  Unicode character with given code
*  \\\ literal backslash (.alert[this one will prove to be especially annoying...])
*  \" literal quote

Others are listed in `?'"'` (the help on the quote function).

---

## Glue with newlines and unicode

```{r}
glue::glue("{names} \n {favorite_food} \U1F600.")
```


---

## Count characters

```{r}
names
nchar(names)
```

---

## Extract substrings

.pull-left[
Extract
```{r}
str_sub(dinner, 1, 11)
```
]
.pull-right[
Replace
```{r}
str_sub(dinner, 
        #space + l-
        nchar(names) + 2, 
        #space + l-i-k-e
        nchar(names) + 6 
) = "demands"
dinner
```
]

---

## split strings

Get a character matrix, padding / collapsing excess fields.
```{r}
str_split_fixed(dinner, " ",  4)
```

```{r}
str_split_fixed(dinner, " ", 6)
```

---

## split strings

Get exactly what you ask for.
```{r}
str_split(dinner, " ")
```

Also recall `tidyr::separate` and `tidyr::separate_rows`.

---

## Other handy low-level string manipulations

*  Change case `str_to_lower()`, `str_to_upper()`, `str_to_title()`
*  Remove trailing/leading `str_trim()` or repeated `str_squish()` whitespace
*  Wrap long lines `stringr::str_wrap()`
*  Truncate `str_trunc()` or abbreviate `base::abbreviate()` long strings.

---

## Application Exercise

.hand[Your turn.] Browse to [Lecture 10 - Text - AE](https://rstudio.cloud/spaces/162296/project/2985078) and complete the exercises in text-munge.Rmd.

---

class: middle

.hand[Regular Expressions]: Uff,  probably wouldn't have done this way in hindsight.

---

## Regular expressions

> Some people, when confronted with a problem, think "I know, I'll use regular expressions." Now they have two problems.
-- Jamie Zawinski (creator of Mozilla)

*  Are like *find-replace*, *wildcards* \* ? on `r emo::ji('pill')`  **and** `r emo::ji('mushroom')`
*  Are found in nearly every computer language
*  Can be just the ticket to solving *some* problems

---
class: bg-green

## Syntax

Write what you want to match (if it's alpha-numeric).

```{r}
lunch = c("one app", "two appetizers", "three apples")
str_view_all(lunch, 'apple')
```

---
class: bg-green

## Match multiple things: wildcard

`.` is a generic wildcard, matches any character.

```{r}
str_view_all(lunch, 'app.')
```

---
class: bg-green

## Match multiple things: character class

`[<set>]` is a character class, matches all characters in `<set>`.  Specify a range of characters with `[<char>-<char>]`.  Invert a class with `[^<set>]`.

```{r}
str_view_all(lunch, 'app[le]')
```

---
class: bg-green

## Match multiple things: disjunction

`(<x>|<y>)` is a disjunction, matches `<x>` or `<y>`.

```{r}
str_view_all(lunch, 'app(le|etizer)s')
```

---
class: bg-green

## Qualifiers modify matches

1. `*` zero or more matches
2. `?` zero or one matches
3. `+` one or more matches
4. `{min,max}` to match between min-max times.

Compare back to `"app."`, which didn't match the first string.
```{r}
str_view_all(lunch, 'app.*')
```

---

## Match without consuming with zero-width identifiers

*  `^` matches a zero-width "character" present at the start of all lines.
*  `$` is the analogous character at the end
*  `\b` is between "words".  

For example, the string:

`red tired`

can be thought as

.darkgreen[^\b]red.darkgreen[\b] .darkgreen[\b]tired.darkgreen[\b$]

---
class: bg-green

## Require word boundary

We must double the `\` to keep R from interpreting it as an escape character.

```{r}
str_view_all("red tired", "\\bred\\b")
```


---
class: bg-green


## Match unconditionally

```{r}
str_view_all("red tired", "red")
```

---


## Using regular expressions

*  Test for an expression `str_detect()`.
*  Return first `str_extract()` or all `str_extract_all()` matching portions of string.
*  Return first `str_match()` or all `str_match_all()` matching portions of string **and capture groups**.
*  Replace first `str_replace()`  or all `str_replace_all()` matching portions of string and capture groups.

---

## `str_detect()`

```{r}
str_detect(string = c("A", "AA", "AB", "B"), 
           pattern = "A")
str_detect(string = lunch, pattern = 'app.')
```

---

## `str_extract()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_extract(string = feline, pattern = "cat")
str_extract_all(string = feline, pattern = "cat")
```

---


## `str_match()`

For simple queries, behaves like `str_extract()`
```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_match(feline, "cat")
```

---

## `str_match()`

But returns **capture groups** `(<expression>)` separately.

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")
str_match(feline, "(\\w*) cat.? (\\w*)")
```

`\w = [A-Za-z0-9_]`, we must double the `\` to keep R from interpreting it as an escape character.

---

## `str_match_all()`
```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_match_all(feline, "(\\w*) cat.? (\\w*)")

```

---

## `str_replace()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_replace(feline, "cat", "murder machine")
```

---

## `str_replace_all()`

```{r}
feline = c("The fur of cats goes by many names.", 
           "Infimum (the cat) is a cat with a most baleful meow.",
           "Dog.")

str_replace_all(feline, "cat", "murder machine")
```

---

## `str_replace()` also can use capture groups

Use `\1` to refer to the first capture group, `\2` for the second, etc.  Note the `\\` because `\` must be escaped in R.
```{r}
str_replace_all(feline, "(\\w*)", "\\1\\1")
```

Actually, any regular expression can use a capture group, both for matching and replacing!
---

## Application exercise

.hand[Your turn]. .alert[Form pairs<sup>1</sup>], and browse again to [Lecture 10 - Text AE](https://rstudio.cloud/spaces/162296/project/2985078) and complete the exercises in regex-text.

You might also experiment with installing `devtools::install_github("gadenbuie/regexplain")`.  See https://www.garrickadenbuie.com/project/regexplain/ for example usage.

.footnote[[1] By this, I highly encourage only one person to code at a time -- the other person should just watch and spout beta.]
---

class: middle

![](https://raw.githubusercontent.com/juliasilge/tidytext/master/man/figures/tidytext.png)

Text mining using `tidytext`

---

## Text mining using `tidytext`

Text is inherently high-dimensional and noisy data.  We could spent weeks on this. Instead, we'll have to be content to know what we don't know:

*  Sampling text data and its potential ascertainment biases
*  Handling non-Roman (ASCII) characters
*  Parsing into tokens
*  Filtering low-content words
*  Dimension reduction, e.g., latent Dirichlet allocation or non-negative matrix factorization
*  Embeddings using pre-trained neural networks

Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/).

---

## Most important functionality

*  `unnest_tokens()` split a string into tokens (words, bi-grams, etc) as a data frame
*  `bind_tf_idf` calculate term and inverse-document frequencies.
*  `cast_sparse` convert to a (sparse) document-term matrix.

---

## Austin vs Kafka
```{r}
library(tidytext)
book_names = tibble(gutenberg_id = c(158, 1342, 5200, 7849),
                    title = c('Emma', 'Pride and prejudice',
                              'Metamorphosis', 'The Trial'))
books = gutenbergr::gutenberg_download(book_names$gutenberg_id) %>% left_join(book_names)
```

.scroll-box-10[
```{r, output.lines = 24}
books %>% group_by(title) %>% slice_head(n=6)
```
]

---

## Get words

```{r}
book_words = unnest_tokens(books, text, output = 'word', drop = TRUE)
book_words
```

---

## Count words by book

```{r}
word_counts = book_words %>%
  group_by(title) %>% count(title, word) %>% 
  arrange(desc(n))
word_counts %>% slice_head(n = 3)
```

---

## Remove "stop" words

Stop words are common, low-semantic value words.  Sometimes useful to remove.

```{r}
word_counts %>% anti_join(get_stopwords()) %>% slice_head(n = 3)
```

---

## Term frequency in Kafka vs Austin

```{r}
total_words = word_counts %>% 
  group_by(title) %>% 
  summarize(total = sum(n))
word_counts = left_join(word_counts, total_words)
word_counts 
```

---

## Term frequency in Kafka vs Austin


```{r plottf, warning = FALSE}
ggplot(word_counts, aes(n/total)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) + 
  facet_wrap(~title, ncol = 2, scales = "free_y") + theme_minimal()
```
---

## Zipf's law
Distributions like those on the previous slide are typical in language.  A classic version of this relationship is called Zipf's law. 

> Zipf's law states that the frequency that a word appears is inversely proportional to its rank. 

---

## Zipf's law

.panelset[

.panel[.panel-name[Code]
```{r freq_by_rank, fig.show='hide'}
freq_by_rank = word_counts %>% 
  group_by(title) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup() 

freq_by_rank %>% 
  ggplot(aes(x = rank, y = `term frequency`, color = title)) + 
  geom_abline(intercept = -0.62, slope = -1, 
              color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() + 
  theme_minimal()
```
]
.panel[.panel-name[Plot]
```{r ref.label = 'freq_by_rank', echo = FALSE}
```
]
]
---

## Sentiment analysis

```{r}
word_sentiments = word_counts %>% 
  left_join(sentiments) %>% #<<
  filter(!is.na(sentiment)) %>% 
  group_by(title) %>% 
  mutate(word_collapse = fct_lump_n(word, n = 10, w = n),
    word_collapse = fct_reorder(word_collapse, n, sum)) %>% 
    select(title, word_collapse, sentiment, n)
word_sentiments
```

---

##  Which is more happy?

```{r}
ggplot(word_sentiments, aes(y = fct_reorder(word_collapse,  n, .fun = sum), x = n, fill = sentiment)) + geom_col() + facet_wrap(~title, scales = 'free_x') + ylab("Word") + xlab("Occurrence") + theme_minimal()
```

---



## Term frequency and inverse document frequency

The inverse document frequency is

$$\text{idf}(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$

The IDF thus ranges from 0 for words that appear in every document up to $log(n)$ for a word unique across documents.

The term frequency is just the word counts, normalized to the number of words per text, so the popular TF-IDF<sup>1</sup> metric is just

$$\text{tf-idf}(\text{term}) = \text{idf}(\text{term}) \times \text{tf}(\text{term})$$
.footnote[[1] Popular, and curiously devoid of an obvious statistical model.  [Some attempts to link to information theory](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Justification_of_idf) have been made.]
---

## Calculate TF-IDF


```{r calc-tf, output.lines = 12}
word_counts = word_counts %>% bind_tf_idf(word, title, n)
word_counts
```

---

## TF-IDF of Kafka and Austen

This words relatively well to identify signature words -- some represent content,  some represent author style (e.g. contractions used by Kafka)

```{r, echo = FALSE}
word_counts %>% group_by(title) %>% slice_max(tf_idf, n = 15) %>% ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~title, ncol = 2, scales = "free") + 
  theme_minimal()
```

---

## Occurrence matrix

Lastly, we might want to convert our counts to an occurrence matrix $\mathbf X = [x_{ij}]$ where $x_{ij}$ is the number of times document $i$ contains term $j$.

Most $x_{ij}$ will be zero, reflecting Zipf's law.  We will almost always want to store it in a special format called a .alert[sparse matrix], that only stores the non-zero entries and their index in the matrix.

---

## `cast_sparse()`

```{r}
X = cast_sparse(word_counts, title, word, n)
class(X)
dim(X)
sum(X>0)
```

This is useful for downstream modeling, such as latent Dirichlet allocation.

---

# Resources

Julia Silge has [one book on classical text mining](https://www.tidytextmining.com/) and [another on machine learning on text](https://smltar.com/).
